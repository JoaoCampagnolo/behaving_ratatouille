{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2e8795",
   "metadata": {},
   "source": [
    "# DLC annotator tutorial\n",
    "Please check this [example](https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/JUPYTER/Demo_yourowndata.ipynb) for more info on this.\n",
    "The DLC workshop on [GitHub](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/master/DLCcourse.md#the-basics-of-computing-in-python-terminal-and-overview-of-dlc) may also provide all the information you may require.\n",
    "This is a notebook intended for my personal use, as I am getting to know and experiment with DLC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a988d7",
   "metadata": {},
   "source": [
    "## Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3713f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import deeplabcut\n",
    "import os\n",
    "from dlc_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c5ea6",
   "metadata": {},
   "source": [
    "## Create a project and configuration path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831f948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'375529': 'blackfly_375529_2021-07-25_7_VIDEO.avi'}\n",
      "['C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\data\\\\375529_7\\\\blackfly_375529_2021-07-25_7_VIDEO.avi']\n"
     ]
    }
   ],
   "source": [
    "mice_id = find_videos(r'C:\\Users\\jhflc\\Documents\\Projects\\KI_article', extension='.avi')\n",
    "print(mice_id)\n",
    "vid_paths = get_paths(['blackfly_375529_2021-07-25_7_VIDEO.avi'],r'C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\data')\n",
    "print(vid_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e8372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\videos\"\n",
      "Created \"C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\labeled-data\"\n",
      "Created \"C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\training-datasets\"\n",
      "Created \"C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\dlc-models\"\n",
      "Copying the videos\n",
      "C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\videos\\blackfly_375529_2021-07-25_7_VIDEO.avi\n",
      "Generated \"C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\config.yaml\"\n",
      "\n",
      "A new project with name POSE-JoaoCampagnolo-2021-11-24 is created at C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='POSE' # Enter the name of your experiment Task\n",
    "experimenter='JoaoCampagnolo' # Enter the name of the experimenter\n",
    "video=['C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\data\\\\375529_7\\\\blackfly_375529_2021-07-25_7_VIDEO.avi'] # Enter the paths of your videos OR FOLDER you want to grab frames from.\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video,copy_videos=True)\n",
    "# NOTE: The function returns the path, where your project is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4067bd2",
   "metadata": {},
   "source": [
    "### Or use pre-existing project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3948311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Note that parameters of this project can be seen at: *Reaching-Mackenzie-2018-08-30/config.yaml*\n",
    "from pathlib import Path\n",
    "\n",
    "#create a variable to set the config.yaml file path:\n",
    "path_config_file = os.path.join(os.getcwd(),\n",
    "                                'C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\scripts\\\\POSE-JoaoCampagnolo-2021-11-24\\\\config.yaml')\n",
    "print(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0adea",
   "metadata": {},
   "source": [
    "## Extract fames from the videos\n",
    "\n",
    "\"A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) ('uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c05ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\jhflc\\Documents\\Projects\\KI_article\\scripts\\POSE-JoaoCampagnolo-2021-11-24\\videos\\blackfly_375529_2021-07-25_7_VIDEO.avi ?\n",
      "yes/noyes\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 1481.5  seconds.\n",
      "Extracting and downsampling... 44445  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44445it [01:31, 487.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#there are other ways to grab frames, such as uniformly; please see the paper:\n",
    "\n",
    "#AUTOMATIC:\n",
    "deeplabcut.extract_frames(path_config_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND/OR:\n",
    "#SELECT RARE EVENTS MANUALLY:\n",
    "%gui wx\n",
    "deeplabcut.extract_frames(path_config_file,'manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29705683",
   "metadata": {},
   "source": [
    "## Label the extracted frames\n",
    "\n",
    "\"Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory **'labeled-data'**. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d763bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e59238",
   "metadata": {},
   "source": [
    "## Check the labels\n",
    "\n",
    "\"Checking if the labels were created and stored correctly is beneficial for training, since labeling is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function `check_labels' to do so. It is used as follows:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89d8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by JoaoCampagnolo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d0611",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "\"This function generates the training data information for network training based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles if they want to benchmark the performance (typcailly, 1 is what you will set, so you pass nothing!).\n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**. For most all use cases we have seen, the defaults are perfectly fine.\n",
    "\n",
    "Now it is the time to start training the network!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca29e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 2, 17,  6, 10,  0,  8,  7, 15,  3, 16, 19, 11, 14,  5, 13, 18, 12,\n",
       "           1,  4]),\n",
       "   array([9])))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)\n",
    "#remember, there are several networks you can pick, the default is resnet-50! Other types: resnet_101, resnet_152, \n",
    "#mobilenet_v2_1.0, mobilenet_v2_0.75, mobilenet_v2_0.5, mobilenet_v2_0.35, efficientnet-b0, efficientnet-b1, \n",
    "#efficientnet-b2, efficientnet-b3, efficientnet-b4, efficientnet-b5, and efficientnet-b6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbd207",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "\"This function trains the network for a specific shuffle of the training dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56978986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3', 'objectA'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'clahe': True,\n",
      " 'claheratio': 0.1,\n",
      " 'crop_pad': 0,\n",
      " 'crop_sampling': 'hybrid',\n",
      " 'crop_size': [400, 400],\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_POSENov24\\\\POSE_JoaoCampagnolo95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'edge': False,\n",
      " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'histeq': True,\n",
      " 'histeqratio': 0.1,\n",
      " 'init_weights': 'C:\\\\Users\\\\jhflc\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'max_shift': 0.4,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_POSENov24\\\\Documentation_data-POSE_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'pre_resize': [],\n",
      " 'project_path': 'C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\scripts\\\\POSE-JoaoCampagnolo-2021-11-24',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'sharpen': False,\n",
      " 'sharpenratio': 0.3,\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\scripts\\\\POSE-JoaoCampagnolo-2021-11-24\\\\dlc-models\\\\iteration-0\\\\POSENov24-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhflc\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ImageNet-pretrained resnet_50\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\scripts\\\\POSE-JoaoCampagnolo-2021-11-24\\\\dlc-models\\\\iteration-0\\\\POSENov24-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'default', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3', 'objectA'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'crop_sampling': 'hybrid', 'crop_size': [400, 400], 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_POSENov24\\\\POSE_JoaoCampagnolo95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': 'C:\\\\Users\\\\jhflc\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'max_shift': 0.4, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_POSENov24\\\\Documentation_data-POSE_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'pre_resize': [], 'project_path': 'C:\\\\Users\\\\jhflc\\\\Documents\\\\Projects\\\\KI_article\\\\scripts\\\\POSE-JoaoCampagnolo-2021-11-24', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 1000 loss: 0.0300 lr: 0.005\n",
      "iteration: 2000 loss: 0.0161 lr: 0.005\n",
      "iteration: 3000 loss: 0.0122 lr: 0.005\n",
      "iteration: 4000 loss: 0.0096 lr: 0.005\n",
      "iteration: 5000 loss: 0.0090 lr: 0.005\n",
      "iteration: 6000 loss: 0.0077 lr: 0.005\n",
      "iteration: 7000 loss: 0.0075 lr: 0.005\n",
      "iteration: 8000 loss: 0.0066 lr: 0.005\n",
      "iteration: 9000 loss: 0.0065 lr: 0.005\n",
      "iteration: 10000 loss: 0.0062 lr: 0.005\n",
      "iteration: 11000 loss: 0.0092 lr: 0.02\n",
      "iteration: 12000 loss: 0.0075 lr: 0.02\n",
      "iteration: 13000 loss: 0.0067 lr: 0.02\n",
      "iteration: 14000 loss: 0.0063 lr: 0.02\n",
      "iteration: 15000 loss: 0.0056 lr: 0.02\n",
      "iteration: 16000 loss: 0.0054 lr: 0.02\n",
      "iteration: 17000 loss: 0.0054 lr: 0.02\n",
      "iteration: 18000 loss: 0.0049 lr: 0.02\n",
      "iteration: 19000 loss: 0.0049 lr: 0.02\n",
      "iteration: 20000 loss: 0.0047 lr: 0.02\n",
      "iteration: 21000 loss: 0.0045 lr: 0.02\n",
      "iteration: 22000 loss: 0.0047 lr: 0.02\n",
      "iteration: 23000 loss: 0.0044 lr: 0.02\n",
      "iteration: 24000 loss: 0.0045 lr: 0.02\n",
      "iteration: 25000 loss: 0.0043 lr: 0.02\n",
      "iteration: 26000 loss: 0.0043 lr: 0.02\n",
      "iteration: 27000 loss: 0.0041 lr: 0.02\n",
      "iteration: 28000 loss: 0.0042 lr: 0.02\n",
      "iteration: 29000 loss: 0.0041 lr: 0.02\n",
      "iteration: 30000 loss: 0.0040 lr: 0.02\n",
      "iteration: 31000 loss: 0.0041 lr: 0.02\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f7055",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "\"This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images) and stores the results as .csv file in a subdirectory under **evaluation-results**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ff811",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10f0ff",
   "metadata": {},
   "source": [
    "## Analyze the videos\n",
    "\n",
    "\"This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "videofile_path = ['videos/video3.avi','videos/video4.avi'] #Enter a folder OR a list of videos to analyze.\n",
    "\n",
    "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c677903",
   "metadata": {},
   "source": [
    "## Extract outlier frames [optional step]\n",
    "\n",
    "\"This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. This step has many options, so please look at:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c02f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,['/videos/video3.avi']) #pass a specific video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d73ad",
   "metadata": {},
   "source": [
    "## Refine Labels [optional step]\n",
    "\n",
    "\"Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cfc49",
   "metadata": {},
   "source": [
    "\"**NOTE**: Afterwards, if you want to look at the adjusted frames, you can load them in the main GUI by running: deeplabcut.label_frames(path_config_file)\n",
    "\n",
    "(you can add a new \"cell\" below to add this code!)\n",
    "\n",
    "#### Once all folders are relabeled, check the labels again! If you are not happy, adjust them in the main GUI:\n",
    "<code>deeplabcut.label_frames(path_config_file)</code>\n",
    "\n",
    "Check Labels:\n",
    "\n",
    "<code>deeplabcut.check_labels(path_config_file)</code>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW, merge this with your original data:\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e82c0",
   "metadata": {},
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "\n",
    "\"Following the refinement of labels and appending them to the original dataset, this creates a new iteration of training dataset. This is automatically set in the config.yaml file, so let's get training!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b800b25",
   "metadata": {},
   "source": [
    "## Create labeled video\n",
    "\n",
    "\"This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides.\n",
    "\n",
    "THIS HAS MANY FUN OPTIONS!\n",
    "\n",
    "<code>deeplabcut.create_labeled_video(config, videos, videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, save_frames=False, Frames2plot=None, delete=False, displayedbodyparts='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False)</code>\n",
    "\n",
    "So please check: <code>deeplabcut.create_labeled_video?</code> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ca284",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c5723",
   "metadata": {},
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "\n",
    "\"This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
